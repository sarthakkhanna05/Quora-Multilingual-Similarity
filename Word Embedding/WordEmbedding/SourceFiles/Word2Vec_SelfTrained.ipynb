{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_english = df[df['language'] == 'English']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>lang_abv</th>\n",
       "      <th>language</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5130fd2cb5</td>\n",
       "      <td>and these comments were considered in formulat...</td>\n",
       "      <td>The rules developed in the interim were put to...</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5b72532a0b</td>\n",
       "      <td>These are issues that we wrestle with in pract...</td>\n",
       "      <td>Practice groups are not permitted to work on t...</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5622f0c60b</td>\n",
       "      <td>you know they can't really defend themselves l...</td>\n",
       "      <td>They can't defend themselves because of their ...</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fdcd1bd867</td>\n",
       "      <td>From Cockpit Country to St. Ann's Bay</td>\n",
       "      <td>From St. Ann's Bay to Cockpit Country.</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7cfb3d272c</td>\n",
       "      <td>Look, it's your skin, but you're going to be i...</td>\n",
       "      <td>The boss will fire you if he sees you slacking...</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12115</th>\n",
       "      <td>2b78e2a914</td>\n",
       "      <td>The results of even the most well designed epi...</td>\n",
       "      <td>All studies have the same amount of uncertaint...</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12116</th>\n",
       "      <td>7e9943d152</td>\n",
       "      <td>But there are two kinds of  the pleasure of do...</td>\n",
       "      <td>But there are two kinds of the pleasure of doi...</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12117</th>\n",
       "      <td>5085923e6c</td>\n",
       "      <td>The important thing is to realize that it's wa...</td>\n",
       "      <td>It cannot be moved, now or ever.</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12118</th>\n",
       "      <td>fc8e2fd1fe</td>\n",
       "      <td>At the west end is a detailed model of the who...</td>\n",
       "      <td>The model temple complex is at the east end.</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12119</th>\n",
       "      <td>44301dfb14</td>\n",
       "      <td>For himself he chose Atat??rk, or Father of th...</td>\n",
       "      <td>Ataturk was the father of the Turkish nation.</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6870 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                            premise  \\\n",
       "0      5130fd2cb5  and these comments were considered in formulat...   \n",
       "1      5b72532a0b  These are issues that we wrestle with in pract...   \n",
       "3      5622f0c60b  you know they can't really defend themselves l...   \n",
       "7      fdcd1bd867              From Cockpit Country to St. Ann's Bay   \n",
       "8      7cfb3d272c  Look, it's your skin, but you're going to be i...   \n",
       "...           ...                                                ...   \n",
       "12115  2b78e2a914  The results of even the most well designed epi...   \n",
       "12116  7e9943d152  But there are two kinds of  the pleasure of do...   \n",
       "12117  5085923e6c  The important thing is to realize that it's wa...   \n",
       "12118  fc8e2fd1fe  At the west end is a detailed model of the who...   \n",
       "12119  44301dfb14  For himself he chose Atat??rk, or Father of th...   \n",
       "\n",
       "                                              hypothesis lang_abv language  \\\n",
       "0      The rules developed in the interim were put to...       en  English   \n",
       "1      Practice groups are not permitted to work on t...       en  English   \n",
       "3      They can't defend themselves because of their ...       en  English   \n",
       "7                 From St. Ann's Bay to Cockpit Country.       en  English   \n",
       "8      The boss will fire you if he sees you slacking...       en  English   \n",
       "...                                                  ...      ...      ...   \n",
       "12115  All studies have the same amount of uncertaint...       en  English   \n",
       "12116  But there are two kinds of the pleasure of doi...       en  English   \n",
       "12117                   It cannot be moved, now or ever.       en  English   \n",
       "12118       The model temple complex is at the east end.       en  English   \n",
       "12119      Ataturk was the father of the Turkish nation.       en  English   \n",
       "\n",
       "       label  \n",
       "0          0  \n",
       "1          2  \n",
       "3          0  \n",
       "7          2  \n",
       "8          1  \n",
       "...      ...  \n",
       "12115      2  \n",
       "12116      0  \n",
       "12117      2  \n",
       "12118      2  \n",
       "12119      0  \n",
       "\n",
       "[6870 rows x 6 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_english #0, 1, 2 corresponds to entailment, neutral, and contradiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-98-84f54348bb09>:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_english['premise'] = [clean_text(sentence) for sentence in df_english['premise']]\n",
      "<ipython-input-98-84f54348bb09>:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_english['hypothesis'] = [clean_text(sentence) for sentence in df_english['hypothesis']]\n"
     ]
    }
   ],
   "source": [
    "# Cleaning each sentence.\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "#will replace the html characters with \" \"\n",
    "    text=re.sub('<.*?>', ' ', text)  \n",
    "    #To remove the punctuations\n",
    "    text = text.translate(str.maketrans(' ',' ',string.punctuation))\n",
    "    #will consider only alphabets and numerics\n",
    "    text = re.sub('[^a-zA-Z]',' ',text)  \n",
    "    #will replace newline with space\n",
    "    text = re.sub(\"\\n\",\" \",text)\n",
    "    #will convert to lower case\n",
    "    text = text.lower()\n",
    "    # will split and join the words\n",
    "    text=' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "df_english['premise'] = [clean_text(sentence) for sentence in df_english['premise']] \n",
    "df_english['hypothesis'] = [clean_text(sentence) for sentence in df_english['hypothesis']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.parsing.preprocessing import remove_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_english['premise_stopw'] = [remove_stopwords(sentence) for sentence in df_english['premise']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_english['hypothesis_stopw'] = [remove_stopwords(sentence) for sentence in df_english['hypothesis']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and these comments were considered in formulating the interim rules\n",
      "the rules developed in the interim were put together with these comments in mind\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df_english['premise'][0])\n",
    "print(df_english['hypothesis'][0])\n",
    "print(df_english['label'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_english['premise_stopw'][0])\n",
    "# print(df_english['hypothesis_stopw'][0])\n",
    "# print(df_english['label'][0])\n",
    "\n",
    "# # I am thinking stop words should not be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-100-b403cfb9c6d3>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_english['tokenized_premise'] = [simple_preprocess(sentence, deacc=True) for sentence in df_english['premise']]\n",
      "<ipython-input-100-b403cfb9c6d3>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_english['tokenized_hypo'] = [simple_preprocess(sentence, deacc=True) for sentence in df_english['hypothesis']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     [and, these, comments, were, considered, in, f...\n",
      "1     [these, are, issues, that, we, wrestle, with, ...\n",
      "3     [you, know, they, cant, really, defend, themse...\n",
      "7           [from, cockpit, country, to, st, anns, bay]\n",
      "8     [look, its, your, skin, but, youre, going, to,...\n",
      "16    [if, you, people, only, knew, how, fatally, ea...\n",
      "17    [my, own, little, corner, of, the, world, poli...\n",
      "18    [life, in, prison, then, hes, available, for, ...\n",
      "19    [the, streets, are, crammed, with, vendors, se...\n",
      "20    [north, of, mytilini, stop, at, the, village, ...\n",
      "Name: tokenized_premise, dtype: object\n",
      "\n",
      "\n",
      "0     [the, rules, developed, in, the, interim, were...\n",
      "1     [practice, groups, are, not, permitted, to, wo...\n",
      "3     [they, cant, defend, themselves, because, of, ...\n",
      "7           [from, st, anns, bay, to, cockpit, country]\n",
      "8     [the, boss, will, fire, you, if, he, sees, you...\n",
      "16    [many, people, have, poisoned, someone, by, mi...\n",
      "17                   [an, example, is, policy, wonking]\n",
      "18    [the, system, is, corrupt, because, he, wont, ...\n",
      "19    [vendors, have, lined, the, streets, with, tor...\n",
      "20    [there, is, nothing, special, to, see, in, the...\n",
      "Name: tokenized_hypo, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "# Tokenize the text column to get the new column 'tokenized_text'\n",
    "df_english['tokenized_premise'] = [simple_preprocess(sentence, deacc=True) for sentence in df_english['premise']] \n",
    "df_english['tokenized_hypo'] = [simple_preprocess(sentence, deacc=True) for sentence in df_english['hypothesis']] \n",
    "print(df_english['tokenized_premise'].head(10))\n",
    "print(\"\\n\")\n",
    "print(df_english['tokenized_hypo'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.parsing.porter import PorterStemmer\n",
    "# porter_stemmer = PorterStemmer()\n",
    "# # Get the stemmed_tokens\n",
    "# df_english['stemmed_premise'] = [[porter_stemmer.stem(word) for word in tokens] for tokens in df_english['tokenized_premise']]\n",
    "# df_english['stemmed_hypo'] = [[porter_stemmer.stem(word) for word in tokens] for tokens in df_english['tokenized_hypo']]\n",
    "# print(df_english['stemmed_premise'].head(10))\n",
    "# print(\"\\n\")\n",
    "# print(df_english['stemmed_hypo'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df_english[['tokenized_premise', 'tokenized_hypo']], \n",
    "                                                    df_english['label'], \n",
    "                                                    test_size=0.20, \n",
    "                                                    random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reset_index()\n",
    "X_test = X_test.reset_index()\n",
    "Y_train = Y_train.to_frame()\n",
    "Y_train = Y_train.reset_index()\n",
    "Y_test = Y_test.to_frame()\n",
    "Y_test = Y_test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1938\n",
      "2    1836\n",
      "1    1722\n",
      "Name: label, dtype: int64\n",
      "0    489\n",
      "1    444\n",
      "2    441\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(Y_train['label'].value_counts())\n",
    "print(Y_test['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Train w2v model with premise text only\n",
    "\n",
    "# stemmed_premise = pd.Series(df_english['stemmed_premise']).values\n",
    "# w2v_model_p = Word2Vec(stemmed_premise, min_count = 1, size = 1000, workers = 3, window = 3, sg = 0) #CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Train w2v model with hypothesis text only\n",
    "\n",
    "# stemmed_hypo = pd.Series(df_english['stemmed_hypo']).values\n",
    "# w2v_model_h = Word2Vec(stemmed_hypo, min_count = 1, size = 1000, workers = 3, window = 3, sg = 0) #CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[and, these, comments, were, considered, in, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[these, are, issues, that, we, wrestle, with, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[you, know, they, cant, really, defend, themse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[from, cockpit, country, to, st, anns, bay]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[look, its, your, skin, but, youre, going, to,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12115</th>\n",
       "      <td>[the, results, of, even, the, most, well, desi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12116</th>\n",
       "      <td>[but, there, are, two, kinds, of, the, pleasur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12117</th>\n",
       "      <td>[the, important, thing, is, to, realize, that,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12118</th>\n",
       "      <td>[at, the, west, end, is, detailed, model, of, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12119</th>\n",
       "      <td>[for, himself, he, chose, atatrk, or, father, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6870 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        tokenized_tokens\n",
       "0      [and, these, comments, were, considered, in, f...\n",
       "1      [these, are, issues, that, we, wrestle, with, ...\n",
       "3      [you, know, they, cant, really, defend, themse...\n",
       "7            [from, cockpit, country, to, st, anns, bay]\n",
       "8      [look, its, your, skin, but, youre, going, to,...\n",
       "...                                                  ...\n",
       "12115  [the, results, of, even, the, most, well, desi...\n",
       "12116  [but, there, are, two, kinds, of, the, pleasur...\n",
       "12117  [the, important, thing, is, to, realize, that,...\n",
       "12118  [at, the, west, end, is, detailed, model, of, ...\n",
       "12119  [for, himself, he, chose, atatrk, or, father, ...\n",
       "\n",
       "[6870 rows x 1 columns]"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train w2v model with both premis and hypothesis text only\n",
    "\n",
    "df1 = df_english[['tokenized_premise']]\n",
    "df1.columns = ['tokenized_tokens']\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[the, rules, developed, in, the, interim, were...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[practice, groups, are, not, permitted, to, wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[they, cant, defend, themselves, because, of, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[from, st, anns, bay, to, cockpit, country]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[the, boss, will, fire, you, if, he, sees, you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12115</th>\n",
       "      <td>[all, studies, have, the, same, amount, of, un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12116</th>\n",
       "      <td>[but, there, are, two, kinds, of, the, pleasur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12117</th>\n",
       "      <td>[it, cannot, be, moved, now, or, ever]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12118</th>\n",
       "      <td>[the, model, temple, complex, is, at, the, eas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12119</th>\n",
       "      <td>[ataturk, was, the, father, of, the, turkish, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6870 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        tokenized_tokens\n",
       "0      [the, rules, developed, in, the, interim, were...\n",
       "1      [practice, groups, are, not, permitted, to, wo...\n",
       "3      [they, cant, defend, themselves, because, of, ...\n",
       "7            [from, st, anns, bay, to, cockpit, country]\n",
       "8      [the, boss, will, fire, you, if, he, sees, you...\n",
       "...                                                  ...\n",
       "12115  [all, studies, have, the, same, amount, of, un...\n",
       "12116  [but, there, are, two, kinds, of, the, pleasur...\n",
       "12117             [it, cannot, be, moved, now, or, ever]\n",
       "12118  [the, model, temple, complex, is, at, the, eas...\n",
       "12119  [ataturk, was, the, father, of, the, turkish, ...\n",
       "\n",
       "[6870 rows x 1 columns]"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df_english[['tokenized_hypo']]\n",
    "df2.columns = ['tokenized_tokens']\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[and, these, comments, were, considered, in, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[these, are, issues, that, we, wrestle, with, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[you, know, they, cant, really, defend, themse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[from, cockpit, country, to, st, anns, bay]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[look, its, your, skin, but, youre, going, to,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13735</th>\n",
       "      <td>[all, studies, have, the, same, amount, of, un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13736</th>\n",
       "      <td>[but, there, are, two, kinds, of, the, pleasur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13737</th>\n",
       "      <td>[it, cannot, be, moved, now, or, ever]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13738</th>\n",
       "      <td>[the, model, temple, complex, is, at, the, eas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13739</th>\n",
       "      <td>[ataturk, was, the, father, of, the, turkish, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13740 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        tokenized_tokens\n",
       "0      [and, these, comments, were, considered, in, f...\n",
       "1      [these, are, issues, that, we, wrestle, with, ...\n",
       "2      [you, know, they, cant, really, defend, themse...\n",
       "3            [from, cockpit, country, to, st, anns, bay]\n",
       "4      [look, its, your, skin, but, youre, going, to,...\n",
       "...                                                  ...\n",
       "13735  [all, studies, have, the, same, amount, of, un...\n",
       "13736  [but, there, are, two, kinds, of, the, pleasur...\n",
       "13737             [it, cannot, be, moved, now, or, ever]\n",
       "13738  [the, model, temple, complex, is, at, the, eas...\n",
       "13739  [ataturk, was, the, father, of, the, turkish, ...\n",
       "\n",
       "[13740 rows x 1 columns]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_text_tokens = pd.concat([df1, df2], axis=0, ignore_index=True)\n",
    "full_text_tokens.reset_index() \n",
    "full_text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_text = pd.Series(full_text_tokens['tokenized_tokens']).values\n",
    "w2v_model_full = Word2Vec(stemmed_text, min_count = 1, size = 1000, workers = 3, window = 7, sg = 1) #Skip gram window 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['vector_p'] = [[] for i in range(5496)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['vector_h'] = [[] for i in range(5496)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['difference'] = [[] for i in range(5496)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>tokenized_premise</th>\n",
       "      <th>tokenized_hypo</th>\n",
       "      <th>vector_p</th>\n",
       "      <th>vector_h</th>\n",
       "      <th>difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3690</td>\n",
       "      <td>[among, the, sights, in, beziers, are, the, an...</td>\n",
       "      <td>[there, is, nothing, interesting, about, bezie...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8344</td>\n",
       "      <td>[in, april, the, sultans, armies, massed, outs...</td>\n",
       "      <td>[there, were, ten, times, as, many, of, the, s...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5426</td>\n",
       "      <td>[when, the, two, nations, divided, it, up, fra...</td>\n",
       "      <td>[france, ended, up, not, getting, the, salt, p...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6436</td>\n",
       "      <td>[over, most, of, the, and, the, us, was, able,...</td>\n",
       "      <td>[the, us, could, invest, more, than, it, saved...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1794</td>\n",
       "      <td>[in, my, crossfire, days, was, patronized, eve...</td>\n",
       "      <td>[during, crossfire, even, sam, donaldson, patr...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5491</th>\n",
       "      <td>1596</td>\n",
       "      <td>[and, and, so, you, know, like, every, other, ...</td>\n",
       "      <td>[tickets, to, see, chima, para, diso, cost, mo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5492</th>\n",
       "      <td>9183</td>\n",
       "      <td>[this, makes, it, incumbent, on, the, governme...</td>\n",
       "      <td>[there, is, no, need, for, the, government, to...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5493</th>\n",
       "      <td>7027</td>\n",
       "      <td>[the, governing, statute, provides, that, comm...</td>\n",
       "      <td>[no, one, can, recommend, an, individual, to, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5494</th>\n",
       "      <td>415</td>\n",
       "      <td>[there, are, two, challengers, to, these, top,...</td>\n",
       "      <td>[these, top, dogs, face, two, tough, financial...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5495</th>\n",
       "      <td>9119</td>\n",
       "      <td>[on, naxos, you, can, walk, through, the, pret...</td>\n",
       "      <td>[instead, of, walking, through, the, villages,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5496 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                  tokenized_premise  \\\n",
       "0      3690  [among, the, sights, in, beziers, are, the, an...   \n",
       "1      8344  [in, april, the, sultans, armies, massed, outs...   \n",
       "2      5426  [when, the, two, nations, divided, it, up, fra...   \n",
       "3      6436  [over, most, of, the, and, the, us, was, able,...   \n",
       "4      1794  [in, my, crossfire, days, was, patronized, eve...   \n",
       "...     ...                                                ...   \n",
       "5491   1596  [and, and, so, you, know, like, every, other, ...   \n",
       "5492   9183  [this, makes, it, incumbent, on, the, governme...   \n",
       "5493   7027  [the, governing, statute, provides, that, comm...   \n",
       "5494    415  [there, are, two, challengers, to, these, top,...   \n",
       "5495   9119  [on, naxos, you, can, walk, through, the, pret...   \n",
       "\n",
       "                                         tokenized_hypo vector_p vector_h  \\\n",
       "0     [there, is, nothing, interesting, about, bezie...       []       []   \n",
       "1     [there, were, ten, times, as, many, of, the, s...       []       []   \n",
       "2     [france, ended, up, not, getting, the, salt, p...       []       []   \n",
       "3     [the, us, could, invest, more, than, it, saved...       []       []   \n",
       "4     [during, crossfire, even, sam, donaldson, patr...       []       []   \n",
       "...                                                 ...      ...      ...   \n",
       "5491  [tickets, to, see, chima, para, diso, cost, mo...       []       []   \n",
       "5492  [there, is, no, need, for, the, government, to...       []       []   \n",
       "5493  [no, one, can, recommend, an, individual, to, ...       []       []   \n",
       "5494  [these, top, dogs, face, two, tough, financial...       []       []   \n",
       "5495  [instead, of, walking, through, the, villages,...       []       []   \n",
       "\n",
       "     difference  \n",
       "0            []  \n",
       "1            []  \n",
       "2            []  \n",
       "3            []  \n",
       "4            []  \n",
       "...         ...  \n",
       "5491         []  \n",
       "5492         []  \n",
       "5493         []  \n",
       "5494         []  \n",
       "5495         []  \n",
       "\n",
       "[5496 rows x 6 columns]"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-297-87a50e8e1b65>:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  model_vector_p = (np.mean([w2v_model_full[token] for token in row['tokenized_premise']], axis=0)).tolist()\n",
      "<ipython-input-297-87a50e8e1b65>:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  model_vector_h = (np.mean([w2v_model_full[token] for token in row['tokenized_hypo']], axis=0)).tolist()\n",
      "<ipython-input-297-87a50e8e1b65>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['vector_p'][index] = model_vector_p\n",
      "<ipython-input-297-87a50e8e1b65>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['vector_h'][index] = model_vector_h\n",
      "D:\\Users\\tommy\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "D:\\Users\\tommy\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "<ipython-input-297-87a50e8e1b65>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['vector_p'][index] = [float(0) for i in range(1000)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for index, row in X_train.iterrows():\n",
    "    model_vector_p = (np.mean([w2v_model_full[token] for token in row['tokenized_premise']], axis=0)).tolist()\n",
    "    model_vector_h = (np.mean([w2v_model_full[token] for token in row['tokenized_hypo']], axis=0)).tolist()\n",
    "    \n",
    "    if type(model_vector_p) is list:  \n",
    "        X_train['vector_p'][index] = model_vector_p\n",
    "    else:\n",
    "        X_train['vector_p'][index] = [float(0) for i in range(1000)]\n",
    "        \n",
    "    if type(model_vector_h) is list:  \n",
    "        X_train['vector_h'][index] = model_vector_h\n",
    "    else:\n",
    "        X_train['vector_h'][index] = [float(0) for i in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5496 entries, 0 to 5495\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   index              5496 non-null   int64 \n",
      " 1   tokenized_premise  5496 non-null   object\n",
      " 2   tokenized_hypo     5496 non-null   object\n",
      " 3   vector_p           5496 non-null   object\n",
      " 4   vector_h           5496 non-null   object\n",
      " 5   difference         5496 non-null   object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 257.8+ KB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-299-37257d207415>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['difference'][index] = np.array(X_train['vector_h'][index]) - np.array(X_train['vector_p'][index])\n"
     ]
    }
   ],
   "source": [
    "for index, row in X_train.iterrows():\n",
    "    X_train['difference'][index] = np.array(X_train['vector_h'][index]) - np.array(X_train['vector_p'][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>tokenized_premise</th>\n",
       "      <th>tokenized_hypo</th>\n",
       "      <th>vector_p</th>\n",
       "      <th>vector_h</th>\n",
       "      <th>difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3690</td>\n",
       "      <td>[among, the, sights, in, beziers, are, the, an...</td>\n",
       "      <td>[there, is, nothing, interesting, about, bezie...</td>\n",
       "      <td>[-0.041606295853853226, -0.003971054684370756,...</td>\n",
       "      <td>[-0.03747285157442093, -0.02189548872411251, 0...</td>\n",
       "      <td>[0.004133444279432297, -0.017924434039741755, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8344</td>\n",
       "      <td>[in, april, the, sultans, armies, massed, outs...</td>\n",
       "      <td>[there, were, ten, times, as, many, of, the, s...</td>\n",
       "      <td>[-0.055340807884931564, -0.008214475587010384,...</td>\n",
       "      <td>[-0.02718065120279789, -0.015308087691664696, ...</td>\n",
       "      <td>[0.028160156682133675, -0.007093612104654312, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5426</td>\n",
       "      <td>[when, the, two, nations, divided, it, up, fra...</td>\n",
       "      <td>[france, ended, up, not, getting, the, salt, p...</td>\n",
       "      <td>[-0.0496622771024704, -0.005100652109831572, 0...</td>\n",
       "      <td>[-0.03821417689323425, -0.011763285845518112, ...</td>\n",
       "      <td>[0.011448100209236145, -0.006662633735686541, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6436</td>\n",
       "      <td>[over, most, of, the, and, the, us, was, able,...</td>\n",
       "      <td>[the, us, could, invest, more, than, it, saved...</td>\n",
       "      <td>[-0.05918313190340996, -0.0019176326459273696,...</td>\n",
       "      <td>[-0.0541241317987442, -0.000576299789827317, 0...</td>\n",
       "      <td>[0.005059000104665756, 0.0013413328561000526, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1794</td>\n",
       "      <td>[in, my, crossfire, days, was, patronized, eve...</td>\n",
       "      <td>[during, crossfire, even, sam, donaldson, patr...</td>\n",
       "      <td>[-0.045232996344566345, 0.007638854440301657, ...</td>\n",
       "      <td>[-0.03540067374706268, 0.014226051047444344, 0...</td>\n",
       "      <td>[0.009832322597503662, 0.006587196607142687, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5491</th>\n",
       "      <td>1596</td>\n",
       "      <td>[and, and, so, you, know, like, every, other, ...</td>\n",
       "      <td>[tickets, to, see, chima, para, diso, cost, mo...</td>\n",
       "      <td>[-0.020456423982977867, -0.01864018663764, 0.0...</td>\n",
       "      <td>[-0.028618648648262024, -0.00768239377066493, ...</td>\n",
       "      <td>[-0.008162224665284157, 0.010957792866975069, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5492</th>\n",
       "      <td>9183</td>\n",
       "      <td>[this, makes, it, incumbent, on, the, governme...</td>\n",
       "      <td>[there, is, no, need, for, the, government, to...</td>\n",
       "      <td>[-0.03616825118660927, -0.014820931479334831, ...</td>\n",
       "      <td>[-0.04421054199337959, -0.021644238382577896, ...</td>\n",
       "      <td>[-0.008042290806770325, -0.006823306903243065,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5493</th>\n",
       "      <td>7027</td>\n",
       "      <td>[the, governing, statute, provides, that, comm...</td>\n",
       "      <td>[no, one, can, recommend, an, individual, to, ...</td>\n",
       "      <td>[-0.03911030665040016, -0.004016361199319363, ...</td>\n",
       "      <td>[-0.05981750413775444, 0.00016223634884227067,...</td>\n",
       "      <td>[-0.02070719748735428, 0.004178597548161633, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5494</th>\n",
       "      <td>415</td>\n",
       "      <td>[there, are, two, challengers, to, these, top,...</td>\n",
       "      <td>[these, top, dogs, face, two, tough, financial...</td>\n",
       "      <td>[-0.02810467779636383, -0.02133854851126671, 0...</td>\n",
       "      <td>[-0.02527136728167534, -0.005838391371071339, ...</td>\n",
       "      <td>[0.002833310514688492, 0.01550015714019537, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5495</th>\n",
       "      <td>9119</td>\n",
       "      <td>[on, naxos, you, can, walk, through, the, pret...</td>\n",
       "      <td>[instead, of, walking, through, the, villages,...</td>\n",
       "      <td>[-0.04026837274432182, 0.0027950024232268333, ...</td>\n",
       "      <td>[-0.04827718436717987, -0.0033226320520043373,...</td>\n",
       "      <td>[-0.008008811622858047, -0.006117634475231171,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5496 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                  tokenized_premise  \\\n",
       "0      3690  [among, the, sights, in, beziers, are, the, an...   \n",
       "1      8344  [in, april, the, sultans, armies, massed, outs...   \n",
       "2      5426  [when, the, two, nations, divided, it, up, fra...   \n",
       "3      6436  [over, most, of, the, and, the, us, was, able,...   \n",
       "4      1794  [in, my, crossfire, days, was, patronized, eve...   \n",
       "...     ...                                                ...   \n",
       "5491   1596  [and, and, so, you, know, like, every, other, ...   \n",
       "5492   9183  [this, makes, it, incumbent, on, the, governme...   \n",
       "5493   7027  [the, governing, statute, provides, that, comm...   \n",
       "5494    415  [there, are, two, challengers, to, these, top,...   \n",
       "5495   9119  [on, naxos, you, can, walk, through, the, pret...   \n",
       "\n",
       "                                         tokenized_hypo  \\\n",
       "0     [there, is, nothing, interesting, about, bezie...   \n",
       "1     [there, were, ten, times, as, many, of, the, s...   \n",
       "2     [france, ended, up, not, getting, the, salt, p...   \n",
       "3     [the, us, could, invest, more, than, it, saved...   \n",
       "4     [during, crossfire, even, sam, donaldson, patr...   \n",
       "...                                                 ...   \n",
       "5491  [tickets, to, see, chima, para, diso, cost, mo...   \n",
       "5492  [there, is, no, need, for, the, government, to...   \n",
       "5493  [no, one, can, recommend, an, individual, to, ...   \n",
       "5494  [these, top, dogs, face, two, tough, financial...   \n",
       "5495  [instead, of, walking, through, the, villages,...   \n",
       "\n",
       "                                               vector_p  \\\n",
       "0     [-0.041606295853853226, -0.003971054684370756,...   \n",
       "1     [-0.055340807884931564, -0.008214475587010384,...   \n",
       "2     [-0.0496622771024704, -0.005100652109831572, 0...   \n",
       "3     [-0.05918313190340996, -0.0019176326459273696,...   \n",
       "4     [-0.045232996344566345, 0.007638854440301657, ...   \n",
       "...                                                 ...   \n",
       "5491  [-0.020456423982977867, -0.01864018663764, 0.0...   \n",
       "5492  [-0.03616825118660927, -0.014820931479334831, ...   \n",
       "5493  [-0.03911030665040016, -0.004016361199319363, ...   \n",
       "5494  [-0.02810467779636383, -0.02133854851126671, 0...   \n",
       "5495  [-0.04026837274432182, 0.0027950024232268333, ...   \n",
       "\n",
       "                                               vector_h  \\\n",
       "0     [-0.03747285157442093, -0.02189548872411251, 0...   \n",
       "1     [-0.02718065120279789, -0.015308087691664696, ...   \n",
       "2     [-0.03821417689323425, -0.011763285845518112, ...   \n",
       "3     [-0.0541241317987442, -0.000576299789827317, 0...   \n",
       "4     [-0.03540067374706268, 0.014226051047444344, 0...   \n",
       "...                                                 ...   \n",
       "5491  [-0.028618648648262024, -0.00768239377066493, ...   \n",
       "5492  [-0.04421054199337959, -0.021644238382577896, ...   \n",
       "5493  [-0.05981750413775444, 0.00016223634884227067,...   \n",
       "5494  [-0.02527136728167534, -0.005838391371071339, ...   \n",
       "5495  [-0.04827718436717987, -0.0033226320520043373,...   \n",
       "\n",
       "                                             difference  \n",
       "0     [0.004133444279432297, -0.017924434039741755, ...  \n",
       "1     [0.028160156682133675, -0.007093612104654312, ...  \n",
       "2     [0.011448100209236145, -0.006662633735686541, ...  \n",
       "3     [0.005059000104665756, 0.0013413328561000526, ...  \n",
       "4     [0.009832322597503662, 0.006587196607142687, -...  \n",
       "...                                                 ...  \n",
       "5491  [-0.008162224665284157, 0.010957792866975069, ...  \n",
       "5492  [-0.008042290806770325, -0.006823306903243065,...  \n",
       "5493  [-0.02070719748735428, 0.004178597548161633, -...  \n",
       "5494  [0.002833310514688492, 0.01550015714019537, -0...  \n",
       "5495  [-0.008008811622858047, -0.006117634475231171,...  \n",
       "\n",
       "[5496 rows x 6 columns]"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = X_train[['difference']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_filename = 'train_review_word2vec.csv'\n",
    "with open(word2vec_filename, 'w+') as word2vec_file:\n",
    "    for index, row in X_train_final.iterrows():\n",
    "        line = \",\".join( [str(vector_element) for vector_element in row['difference']] )\n",
    "        word2vec_file.write(line)\n",
    "        word2vec_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_df = pd.read_csv(word2vec_filename, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.004133</td>\n",
       "      <td>-0.017924</td>\n",
       "      <td>-0.000458</td>\n",
       "      <td>-0.012234</td>\n",
       "      <td>-0.013450</td>\n",
       "      <td>0.026058</td>\n",
       "      <td>-0.005402</td>\n",
       "      <td>0.042019</td>\n",
       "      <td>0.025742</td>\n",
       "      <td>0.071090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024719</td>\n",
       "      <td>0.008216</td>\n",
       "      <td>0.049058</td>\n",
       "      <td>0.073169</td>\n",
       "      <td>0.011492</td>\n",
       "      <td>0.016453</td>\n",
       "      <td>-0.042827</td>\n",
       "      <td>-0.050440</td>\n",
       "      <td>-0.046953</td>\n",
       "      <td>0.046376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.028160</td>\n",
       "      <td>-0.007094</td>\n",
       "      <td>0.008630</td>\n",
       "      <td>-0.025215</td>\n",
       "      <td>0.014156</td>\n",
       "      <td>0.016967</td>\n",
       "      <td>-0.017702</td>\n",
       "      <td>-0.009299</td>\n",
       "      <td>0.018359</td>\n",
       "      <td>0.055406</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000471</td>\n",
       "      <td>0.018826</td>\n",
       "      <td>0.017758</td>\n",
       "      <td>-0.003506</td>\n",
       "      <td>-0.009164</td>\n",
       "      <td>0.017529</td>\n",
       "      <td>-0.018782</td>\n",
       "      <td>0.002892</td>\n",
       "      <td>0.000846</td>\n",
       "      <td>0.025329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.011448</td>\n",
       "      <td>-0.006663</td>\n",
       "      <td>-0.013427</td>\n",
       "      <td>0.016475</td>\n",
       "      <td>-0.007418</td>\n",
       "      <td>0.004650</td>\n",
       "      <td>0.001963</td>\n",
       "      <td>0.025377</td>\n",
       "      <td>-0.003794</td>\n",
       "      <td>0.002527</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008093</td>\n",
       "      <td>0.003830</td>\n",
       "      <td>0.014084</td>\n",
       "      <td>-0.006920</td>\n",
       "      <td>0.008054</td>\n",
       "      <td>0.005727</td>\n",
       "      <td>-0.017333</td>\n",
       "      <td>0.004003</td>\n",
       "      <td>-0.001258</td>\n",
       "      <td>0.005550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.005059</td>\n",
       "      <td>0.001341</td>\n",
       "      <td>-0.009286</td>\n",
       "      <td>-0.003490</td>\n",
       "      <td>0.007138</td>\n",
       "      <td>-0.001907</td>\n",
       "      <td>-0.001808</td>\n",
       "      <td>0.003166</td>\n",
       "      <td>0.011655</td>\n",
       "      <td>0.010192</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003372</td>\n",
       "      <td>0.004145</td>\n",
       "      <td>0.026897</td>\n",
       "      <td>0.001819</td>\n",
       "      <td>0.002819</td>\n",
       "      <td>0.006963</td>\n",
       "      <td>0.013770</td>\n",
       "      <td>-0.010112</td>\n",
       "      <td>-0.002618</td>\n",
       "      <td>0.001109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.009832</td>\n",
       "      <td>0.006587</td>\n",
       "      <td>-0.016311</td>\n",
       "      <td>0.014898</td>\n",
       "      <td>-0.010194</td>\n",
       "      <td>-0.011183</td>\n",
       "      <td>0.012595</td>\n",
       "      <td>0.020455</td>\n",
       "      <td>0.006039</td>\n",
       "      <td>0.003262</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011713</td>\n",
       "      <td>0.002869</td>\n",
       "      <td>0.011386</td>\n",
       "      <td>-0.006392</td>\n",
       "      <td>0.010916</td>\n",
       "      <td>0.001611</td>\n",
       "      <td>0.011989</td>\n",
       "      <td>-0.000393</td>\n",
       "      <td>0.001154</td>\n",
       "      <td>-0.002841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5491</th>\n",
       "      <td>-0.008162</td>\n",
       "      <td>0.010958</td>\n",
       "      <td>-0.013914</td>\n",
       "      <td>0.004735</td>\n",
       "      <td>-0.001258</td>\n",
       "      <td>-0.016867</td>\n",
       "      <td>0.012103</td>\n",
       "      <td>-0.032372</td>\n",
       "      <td>-0.001511</td>\n",
       "      <td>-0.032048</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015668</td>\n",
       "      <td>0.020368</td>\n",
       "      <td>-0.033421</td>\n",
       "      <td>-0.061742</td>\n",
       "      <td>0.004347</td>\n",
       "      <td>-0.012853</td>\n",
       "      <td>0.061575</td>\n",
       "      <td>0.016641</td>\n",
       "      <td>0.028621</td>\n",
       "      <td>-0.047733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5492</th>\n",
       "      <td>-0.008042</td>\n",
       "      <td>-0.006823</td>\n",
       "      <td>0.009128</td>\n",
       "      <td>-0.010005</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>0.004985</td>\n",
       "      <td>-0.011908</td>\n",
       "      <td>-0.018444</td>\n",
       "      <td>0.016395</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010998</td>\n",
       "      <td>0.001697</td>\n",
       "      <td>0.002548</td>\n",
       "      <td>-0.006516</td>\n",
       "      <td>-0.010537</td>\n",
       "      <td>0.006366</td>\n",
       "      <td>-0.005314</td>\n",
       "      <td>0.005958</td>\n",
       "      <td>0.005359</td>\n",
       "      <td>0.002370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5493</th>\n",
       "      <td>-0.020707</td>\n",
       "      <td>0.004179</td>\n",
       "      <td>-0.020527</td>\n",
       "      <td>-0.007293</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>-0.000805</td>\n",
       "      <td>-0.001848</td>\n",
       "      <td>0.006987</td>\n",
       "      <td>0.032347</td>\n",
       "      <td>0.015042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015425</td>\n",
       "      <td>0.013031</td>\n",
       "      <td>0.017382</td>\n",
       "      <td>0.020884</td>\n",
       "      <td>0.007595</td>\n",
       "      <td>0.020241</td>\n",
       "      <td>0.010363</td>\n",
       "      <td>-0.023522</td>\n",
       "      <td>-0.019067</td>\n",
       "      <td>0.000868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5494</th>\n",
       "      <td>0.002833</td>\n",
       "      <td>0.015500</td>\n",
       "      <td>-0.018369</td>\n",
       "      <td>0.017540</td>\n",
       "      <td>-0.004736</td>\n",
       "      <td>0.007439</td>\n",
       "      <td>0.019580</td>\n",
       "      <td>0.016219</td>\n",
       "      <td>0.002396</td>\n",
       "      <td>-0.003629</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004227</td>\n",
       "      <td>-0.009584</td>\n",
       "      <td>-0.007454</td>\n",
       "      <td>-0.004102</td>\n",
       "      <td>0.019190</td>\n",
       "      <td>0.007424</td>\n",
       "      <td>0.025048</td>\n",
       "      <td>-0.006686</td>\n",
       "      <td>0.007501</td>\n",
       "      <td>-0.001669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5495</th>\n",
       "      <td>-0.008009</td>\n",
       "      <td>-0.006118</td>\n",
       "      <td>-0.011557</td>\n",
       "      <td>-0.011730</td>\n",
       "      <td>0.016712</td>\n",
       "      <td>0.014065</td>\n",
       "      <td>-0.000073</td>\n",
       "      <td>-0.001358</td>\n",
       "      <td>0.011065</td>\n",
       "      <td>0.017316</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011848</td>\n",
       "      <td>0.007626</td>\n",
       "      <td>0.007895</td>\n",
       "      <td>0.016579</td>\n",
       "      <td>0.008037</td>\n",
       "      <td>0.004226</td>\n",
       "      <td>0.007552</td>\n",
       "      <td>-0.001247</td>\n",
       "      <td>-0.016500</td>\n",
       "      <td>0.013236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5496 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0     0.004133 -0.017924 -0.000458 -0.012234 -0.013450  0.026058 -0.005402   \n",
       "1     0.028160 -0.007094  0.008630 -0.025215  0.014156  0.016967 -0.017702   \n",
       "2     0.011448 -0.006663 -0.013427  0.016475 -0.007418  0.004650  0.001963   \n",
       "3     0.005059  0.001341 -0.009286 -0.003490  0.007138 -0.001907 -0.001808   \n",
       "4     0.009832  0.006587 -0.016311  0.014898 -0.010194 -0.011183  0.012595   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5491 -0.008162  0.010958 -0.013914  0.004735 -0.001258 -0.016867  0.012103   \n",
       "5492 -0.008042 -0.006823  0.009128 -0.010005  0.016100  0.004985 -0.011908   \n",
       "5493 -0.020707  0.004179 -0.020527 -0.007293  0.004300 -0.000805 -0.001848   \n",
       "5494  0.002833  0.015500 -0.018369  0.017540 -0.004736  0.007439  0.019580   \n",
       "5495 -0.008009 -0.006118 -0.011557 -0.011730  0.016712  0.014065 -0.000073   \n",
       "\n",
       "           7         8         9    ...       990       991       992  \\\n",
       "0     0.042019  0.025742  0.071090  ...  0.024719  0.008216  0.049058   \n",
       "1    -0.009299  0.018359  0.055406  ... -0.000471  0.018826  0.017758   \n",
       "2     0.025377 -0.003794  0.002527  ...  0.008093  0.003830  0.014084   \n",
       "3     0.003166  0.011655  0.010192  ...  0.003372  0.004145  0.026897   \n",
       "4     0.020455  0.006039  0.003262  ...  0.011713  0.002869  0.011386   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5491 -0.032372 -0.001511 -0.032048  ... -0.015668  0.020368 -0.033421   \n",
       "5492 -0.018444  0.016395  0.004345  ... -0.010998  0.001697  0.002548   \n",
       "5493  0.006987  0.032347  0.015042  ...  0.015425  0.013031  0.017382   \n",
       "5494  0.016219  0.002396 -0.003629  ...  0.004227 -0.009584 -0.007454   \n",
       "5495 -0.001358  0.011065  0.017316  ...  0.011848  0.007626  0.007895   \n",
       "\n",
       "           993       994       995       996       997       998       999  \n",
       "0     0.073169  0.011492  0.016453 -0.042827 -0.050440 -0.046953  0.046376  \n",
       "1    -0.003506 -0.009164  0.017529 -0.018782  0.002892  0.000846  0.025329  \n",
       "2    -0.006920  0.008054  0.005727 -0.017333  0.004003 -0.001258  0.005550  \n",
       "3     0.001819  0.002819  0.006963  0.013770 -0.010112 -0.002618  0.001109  \n",
       "4    -0.006392  0.010916  0.001611  0.011989 -0.000393  0.001154 -0.002841  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5491 -0.061742  0.004347 -0.012853  0.061575  0.016641  0.028621 -0.047733  \n",
       "5492 -0.006516 -0.010537  0.006366 -0.005314  0.005958  0.005359  0.002370  \n",
       "5493  0.020884  0.007595  0.020241  0.010363 -0.023522 -0.019067  0.000868  \n",
       "5494 -0.004102  0.019190  0.007424  0.025048 -0.006686  0.007501 -0.001669  \n",
       "5495  0.016579  0.008037  0.004226  0.007552 -0.001247 -0.016500  0.013236  \n",
       "\n",
       "[5496 rows x 1000 columns]"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(max_iter=4000)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Let us try linear SVC model\n",
    "SVC = LinearSVC(max_iter=4000)\n",
    "SVC.fit(word2vec_df, Y_train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=200)"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf2 = RandomForestClassifier(n_estimators=200)\n",
    "clf2.fit(word2vec_df, Y_train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-307-25cfc94a47b0>:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  model_vector_p = (np.mean([w2v_model_full[token] for token in row['tokenized_premise']], axis=0)).tolist()\n",
      "<ipython-input-307-25cfc94a47b0>:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  model_vector_h = (np.mean([w2v_model_full[token] for token in row['tokenized_hypo']], axis=0)).tolist()\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "test_features_p = []\n",
    "test_features_h = []\n",
    "test_features_difference = []\n",
    "for index, row in X_test.iterrows():\n",
    "    model_vector_p = (np.mean([w2v_model_full[token] for token in row['tokenized_premise']], axis=0)).tolist()\n",
    "    model_vector_h = (np.mean([w2v_model_full[token] for token in row['tokenized_hypo']], axis=0)).tolist()\n",
    "    if type(model_vector_p) is list:\n",
    "        test_features_p.append(model_vector_p)\n",
    "    else:\n",
    "        test_features_p.append(np.array([0 for i in range(1000)]))\n",
    "        \n",
    "    if type(model_vector_h) is list:\n",
    "        test_features_h.append(model_vector_h)\n",
    "    else:\n",
    "        test_features_h.append(np.array([0 for i in range(1000)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_features_h)):\n",
    "    difference = np.array(test_features_h[i]) - np.array(test_features_p[i])\n",
    "    test_features_difference.append(difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.51      0.48       489\n",
      "           1       0.41      0.39      0.40       444\n",
      "           2       0.42      0.41      0.42       441\n",
      "\n",
      "    accuracy                           0.44      1374\n",
      "   macro avg       0.43      0.43      0.43      1374\n",
      "weighted avg       0.44      0.44      0.44      1374\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predictions_word2vec = clf2.predict(test_features_difference)\n",
    "print(classification_report(Y_test['label'],test_predictions_word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.52      0.47       489\n",
      "           1       0.40      0.28      0.33       444\n",
      "           2       0.46      0.50      0.48       441\n",
      "\n",
      "    accuracy                           0.43      1374\n",
      "   macro avg       0.43      0.43      0.42      1374\n",
      "weighted avg       0.43      0.43      0.43      1374\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predictions_word2vec_svc = SVC.predict(test_features_difference)\n",
    "print(classification_report(Y_test['label'],test_predictions_word2vec_svc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
